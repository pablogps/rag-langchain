{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1) Setup + configuración",
   "id": "1298fee8d1fcfe40"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "assert OPENAI_API_KEY, \"Missing OPENAI_API_KEY at .env\"\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "assert DATA_DIR.exists(), \"data/ folder does not exist!\"\n",
    "\n",
    "print(\"✅ Environment OK\")"
   ],
   "id": "bdd40ce7da0e1b62"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "BOE_PDF = DATA_DIR / \"BOE-A-2020-8608.pdf\"\n",
    "SENTINEL_PDF = DATA_DIR / \"sentinel_secure_services_maual_operativo.pdf\"  # noqa\n",
    "\n",
    "print(\"BOE exists:\", BOE_PDF.exists())\n",
    "print(\"Sentinel exists:\", SENTINEL_PDF.exists())"
   ],
   "id": "89e4d446b9a19b8f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2) Carga + extracción",
   "id": "4f5e825a1ef37b09"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "\n",
    "def load_pdf(path: Path) -> list[Document]:\n",
    "    loader = PyPDFLoader(str(path))\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "\n",
    "docs_boe = load_pdf(BOE_PDF) if BOE_PDF.exists() else []\n",
    "docs_sentinel = load_pdf(SENTINEL_PDF) if SENTINEL_PDF.exists() else []\n",
    "\n",
    "docs = docs_boe\n",
    "print(f\"Pages loaded: {len(docs)}\")\n",
    "print(f\"Example metadata: {docs[0].metadata}\" if docs else None)\n",
    "print(f\"Sample text: {docs[0].page_content[:400]} [...]\" if docs else \"No docs\")"
   ],
   "id": "354989a22d463623"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def doc_stats(documents: list[Document]) -> tuple[int, int]:\n",
    "    total_characters = sum(len(d.page_content) for d in documents)\n",
    "    average_characters = total_characters / max(1, len(documents))\n",
    "    return total_characters, average_characters\n",
    "\n",
    "\n",
    "total_chars, avg_chars = doc_stats(docs)\n",
    "print(f\"Total characters: {total_chars:_}\\nAvg characters per page: {avg_chars:_.0f}\")"
   ],
   "id": "9552429a974cb4d7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3) Chunking",
   "id": "c04f5e8181c7e30e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "def split_docs(documents: list[Document], chunk_size: int = 800, chunk_overlap: int = 120) -> list[Document]:\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "    )\n",
    "    return splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "CHUNK_SIZE = 100\n",
    "CHUNK_OVERLAP = 10\n",
    "\n",
    "docs_chunks = split_docs(docs, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "\n",
    "print(f\"{len(docs_chunks)} chunks (size {CHUNK_SIZE}, overlap {CHUNK_OVERLAP})\")"
   ],
   "id": "e5d1e97e225e19fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import textwrap\n",
    "\n",
    "\n",
    "def preview_chunks(chunks: list[Document], n: int = 2, width: int = 100, max_chars: int = 800) -> None:\n",
    "    for i in range(min(n, len(chunks))):\n",
    "        md = chunks[i].metadata\n",
    "        txt = chunks[i].page_content.strip().replace(\"\\n\", \" \")\n",
    "        print(f\"\\n—Chunk {i} | source={md.get('source')}; page={md.get('page')}; len={len(chunks[i].page_content):_}\")\n",
    "        print(textwrap.fill(txt[:max_chars], width=width))\n",
    "\n",
    "\n",
    "preview_chunks(docs_chunks, n=2, max_chars=800)"
   ],
   "id": "be0d0ce9b351c323"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4) Embeddings + vector store",
   "id": "f6e77163a4c98ec3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "\n",
    "EMBED_MODEL = \"text-embedding-3-small\"\n",
    "\n",
    "\n",
    "def build_chroma(chunks: list[Document], persist_dir=\"chroma_boe\", collection=\"boe\") -> Chroma:\n",
    "    embeddings = OpenAIEmbeddings(model=EMBED_MODEL)\n",
    "    db = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_dir,\n",
    "        collection_name=collection,\n",
    "    )\n",
    "    return db\n",
    "\n",
    "\n",
    "db_chunks = build_chroma(docs_chunks, persist_dir=f\"chroma_{CHUNK_SIZE}\", collection=f\"size_{CHUNK_SIZE}\")\n",
    "print(f\"✅ Chroma built (size_{CHUNK_SIZE}).\")"
   ],
   "id": "e4f3daee45f1945c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5) Retrieval",
   "id": "634da0c874326623"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def retrieve_with_scores(db: Chroma, query_: str, n_results: int = 4) -> list[tuple[Document, float]]:\n",
    "    return db.similarity_search_with_score(query_, k=n_results)\n",
    "\n",
    "\n",
    "def print_chroma_hits(hits: list[tuple[Document, float]], max_chars: int = 600, width: int = 110) -> None:\n",
    "    print(f\"Score: less is better for cosine-distance based search.\")\n",
    "    for i, (doc, score) in enumerate(hits):\n",
    "        md = doc.metadata\n",
    "        print(f\"\\n#{i} score={score:.4f}; page={md.get('page')}; source={md.get('source')}\")\n",
    "        print(textwrap.fill(doc.page_content[:max_chars].replace(\"\\n\", \" \"), width=width))\n",
    "\n",
    "\n",
    "query = \"¿Cuál es el porcentaje máximo de subvención para proyectos en fase comercial?\"  # noqa\n",
    "results = retrieve_with_scores(db_chunks, query, n_results=5)\n",
    "print_chroma_hits(results, max_chars=600)"
   ],
   "id": "8ddc6bc64817c172"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Experimento 1\n",
    "\n",
    "El pipeline funciona tal como esperamos, pero los resultados no parecen buenos... ¿por qué?\n",
    "\n",
    "Prueba con 3 o 4 combinaciones de CHUNK_SIZE y CHUNK_OVERLAP. Analiza los fragmentos generados, los top-5 fragmentos recuperados.\n",
    "\n",
    "¿Qué impacto crees que tendrá esta decisión en las respuestas finales del sistema?"
   ],
   "id": "49bcc8190b77ff42"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6) Generación",
   "id": "9b83e77478512897"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0.0)\n",
    "\n",
    "\n",
    "def answer_no_context(question_: str) -> str:\n",
    "    return llm.invoke(question_).content\n",
    "\n",
    "\n",
    "def answer_with_context(question_: str, retrieved_docs: list[Document]) -> str:\n",
    "    context = \"\\n\\n\".join(\n",
    "        f\"[source={d.metadata.get('source')}; page={d.metadata.get('page')}]\\n{d.page_content}\"\n",
    "        for d in retrieved_docs\n",
    "    )\n",
    "\n",
    "    # noinspection SpellCheckingInspection\n",
    "    prompt = f\"\"\"Contexto: {context}\\nPregunta: {question_}\"\"\"\n",
    "    return llm.invoke(prompt).content"
   ],
   "id": "a9b3674b981259ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "question = \"¿Cuál es el porcentaje máximo de subvención para proyectos en fase comercial?\"  # noqa\n",
    "\n",
    "print(\"=== No context ===\")\n",
    "print(answer_no_context(question))\n",
    "\n",
    "print(f\"\\n=== With RAG context (chunk size {CHUNK_SIZE}) ===\")\n",
    "retrieved = [doc for doc, _score in retrieve_with_scores(db_chunks, question, n_results=1)]\n",
    "print(answer_with_context(question, retrieved))"
   ],
   "id": "cea69bce0137e786"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Experimento 2\n",
    "\n",
    "Acabamos de ver que elegir un buen CHUNK_SIZE es importante. Sin embargo, en la práctica intentamos tener sistemas más robustos incluso para parámetros subóptimos.\n",
    "\n",
    "Prueba a cambiar el valor de n_results para distintos valores de CHUNK_SIZE y observa los resultados. ¿Crees que existe una correlación entre CHUNK_SIZE y n_results?"
   ],
   "id": "7bc2fb655d93edd2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Experimento 3\n",
    "\n",
    "Hemos probado a trabajar con un prompt muy básico:\n",
    "\n",
    "```prompt = f\"\"\"Contexto: {context}\\nPregunta: {question_}\"\"\"```\n",
    "\n",
    "Prueba a lanzar una pregunta claramente fuera del contexto del documento de nuestra base documental: ¿qué ocurre?\n",
    "\n",
    "¿Puedes conseguir que solo responda cuando la respuesta se encuentre en el contexto? (¿Es este el comportamiento idóneo?)\n",
    "\n",
    "¿Puedes conseguir que la respuesta sea más verbosa? ¿Y más esquemática pero citando detalles de la fuente utilizada?"
   ],
   "id": "5a916072340daa39"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
